<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0047)https://hellomlai2017.github.io// -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>IEEE Workshop on Machine Learning and Artificial Intelligence for Multimedia Creation</title>
<style type="text/css">
<!--
#apDiv1 {
	position:absolute;
	width:726px;
	height:113px;
	z-index:2;
	left: 388px;
	top: 35px;
}
.STYLE1 {
	color: #FFFFFF;
	font-size: large;
}
.STYLE2 {
	font-size: xx-large;
	color: #FFFFFF;
	font-weight: bold;
}
#apDiv2 {  
	position:absolute;
	width:242px;
	height:315px;
	z-index:2;
	left: 19px;
	top: 450px;
}
.STYLE5 {color: #008888}
a:link {
	color: #0000ff;
	text-decoration: none;
}
a:visited {
	text-decoration: none;
	color: #0000ff;
}
a:hover {
	text-decoration: underline;
}
a:active {
	text-decoration: none;
}
#apDiv3 {
	position:absolute;
	width:316px;
	height:167px;
	z-index:3;
	left: 6px;
	top: 986px;
}
#apDiv4 {
	position:absolute;
	width:300px;
	height:150px;
	z-index:1;
	left: 7px;
	top: 800px;
}
#apDiv5 {
	position:absolute;
	width:303px;
	height:100px;
	z-index:4;
	left: 6px;
	top: 920px;
}
#apDiv6 {
	position:absolute;
	width:780px;
	height:1569px;
	z-index:5;
	left: 344px;
	top: 450px;
	text-justify:inter-ideograph;
	text-align:justify;
}
.STYLE6 {font-family: Arial, Helvetica, sans-serif;}
.STYLE7 {font-family: Arial, Helvetica, sans-serif; font-weight: bold; }
.smj {font-family: Arial, Helvetica, sans-serif;color:#0000ff}
#apDiv7 {
	position:absolute;
	width:366px;
	height:23px;
	z-index:6;
	left: -282px;
	top: -64px;
}
.STYLE8 {
	color: #FFFFFF;
	font-family: Arial, Helvetica, sans-serif;
	font-weight: bold;
}
#apDiv8 {
	position:absolute;
	width:404px;
	height:79px;
	z-index:6;
	left: 710px;
	top: 95px;
}
#a{
   color:#FF0000;
   } 
#myspan{
color:#FF9900;
}
.myspan1{
color:red;
}
table
  {
  border-collapse:collapse;
  text-align:center;
  
  }
table,th, td
  {
  border: 1px solid black;
  padding-top:10px;
  padding-bottom:10px;
  }
 
-->
</style>
<meta name="Keywords" content="IEEE Workshop on Machine Learning and Artificial Intelligence for Multimedia Creation">
</head>
<body>
<img src="./image/top.jpg" width="1200" height="400" border="0" usemap="#Map">
<map name="Map" id="Map">
  <area shape="rect" coords="5,5,1200,400" href="https://hellomlai2017.github.io/">
</map>
<div id="apDiv2">
  <p><span class="STYLE5"><span class="STYLE6"><a href=https://hellomlai2017.github.io//#0"></a></span></span></p> 
  <p class="STYLE6">&nbsp;<span class="STYLE5">&nbsp;<a href="https://hellomlai2017.github.io//#1">Motivation</a> </span></p>
  <p class="STYLE6">&nbsp;&nbsp;<span class="STYLE5"><a href="https://hellomlai2017.github.io//#2">Topics</a></span>&nbsp; </p>
  <p class="STYLE6">&nbsp;&nbsp;<span class="STYLE5"><a href="https://hellomlai2017.github.io//#3">Paper submission instructions</a></span></p>  
  <p><span class="STYLE6">&nbsp;&nbsp;<span class="STYLE5"><a href="https://hellomlai2017.github.io//#4">Important&nbsp;Dates</a></span></span></p>
  <p><span class="STYLE6">&nbsp;&nbsp;<span class="STYLE5"><a href="https://hellomlai2017.github.io//#5">Review&nbsp;procedure</a></span></span></p>
  <p class="STYLE6">&nbsp;&nbsp;<span class="STYLE5"><a href="https://hellomlai2017.github.io//#6">Workshop&nbsp;Organizers</a></span></p>
  <p><span class="STYLE6">&nbsp;&nbsp;<span class="STYLE5"><a href="https://hellomlai2017.github.io//#7">Workshop&nbsp;Chairs</a></span></span></p>
  <p><span class="STYLE6">&nbsp;&nbsp;<span class="STYLE5"><a href="https://hellomlai2017.github.io//#8">Technical&nbsp;Program&nbsp;Committee</a></span></span></p>
  <p><span class="STYLE6">&nbsp;&nbsp;<span class="STYLE5"><a href="https://hellomlai2017.github.io//#9">Kenote&nbsp;Talk</a></span></span></p>
</div>
<div id="apDiv6">
  <h2 align="center" class="smj"> <a name="0" id="0"></a>IEEE Workshop on Machine Learning and Artificial Intelligence for Multimedia Creation</h2> 
 
  <p class="STYLE6"> <b>July 23-27, 2018 @ San Diego, CA, USA </b></p>
  <p class="STYLE6"> In conjunction with the <a name="0" id="0" href="http://www.icme2018.org/" target="_blank">2018 IEEE International Conference on Multimedia and Expo (ICME) </a></p> 
  
   <p class="STYLE6"> All accepted papers which are registered by the author deadline and presented at the conference will be included in IEEE Xplore.</p> 
  
  <p align="center" class="STYLE6">&nbsp;</p>
  
  
  
    <h3 class="STYLE6"><a name="10" id="10"></a>Schedule</h3>
	<strong>Time:</strong> 8:30 - 12:30 <br>
	<strong>Date:</strong> Friday, July 27, 2018  <br>
	<strong>Room:</strong> Milos <br>
	<br>

        <table border="1" cellspacing="0">
            <tr>
                <th width="100">Time</th>
                <th>Title</th>
                <th>Presenter/Author</th>
            </tr>
            <tr>
                <td>8:30 – 8:40</td>
                <td>Opening remarks</td>
                <td>Dr. Sijia Liu</td>
            </tr>
            <tr>
                <td>8:40 – 9:20</td>
                <td>Keynote Talk: A Multi-task Learning framework for Head Pose Estimation and Actor-Action Semantic Video Segmentation</td>
                <td>Prof. Yan Yan</td>
            </tr>
            <tr>
                <td>9:21 – 9:38</td>
                <td>Paper #46 Video Super Resolution Based on Deep Convolution Neural Network with Two-stage Motion Compensation</td>
                <td>Haoyu Ren, Mostafa El Khamy, Jungwon Lee </td>
            </tr>
            <tr>
                <td>9:39 – 9:56</td>
                <td>Paper #55 A Fast No-reference Screen Content Image Quality Prediction using Convolutional Neural Networks</td>
                <td>Zhengxue Cheng, Masaru Takeuchi, Kenji Kanai, Jiro Katto</td>
            </tr>
            <tr>
                <td>9:57 – 10:14</td>
                <td>Paper #57 An Enhanced Deep Convolutional Neural Network for Person Re-identification</td>
                <td>Tiansheng Guo, Dongfei Wang, Zhuqing Jiang, Aidong Men, Yun Zhou</td>
            </tr>
            <tr>
                <td>10:15 – 10:32</td>
                <td>Paper #71 Single Image Haze Removal via Joint Estimation of Detail and Transmission</td>
                <td>Shengdong Zhang, Yao Jian, Wenqi Ren</td>
            </tr>
			
            <tr>
                <th colspan="3">Coffee Break (10:33 – 10:45)</th>
            </tr>
            <tr>
                <td>10:46 – 11:03</td>
                <td>Paper #82 Deep Global and Local Saliency Learning with New Re-ranking for Person Re-Identification</td>
                <td>Wei Fei, Zhicheng Zhao, Fei Su</td>
            </tr>
            <tr>
                <td>11:04 – 11:21</td>
                <td>Paper #95 Hierarchical Learning of Sparse Image Representations using Steered Mixture of Experts</td>
                <td>Rolf Jongebloed, Ruben Verhack, Lieven Lange, Thomas Sikora</td>
            </tr>
            <tr>
                <td>11:22 – 11:39</td>
                <td>Paper #123 HDR Image Reconstruction Using Locally Weighted Linear Regression</td>
                <td>Xiaofen Li,Yongqing Huo </td>
            </tr>
            <tr>
                <td>11:40 – 11:57</td>
                <td>Paper #124 Supporting Collaboration Among Cyber Security Analysts Through Visualizing their Analytical Reasoning Processes</td>
                <td>Lindsey Thomas, Adam Vaughan, Zachary Courtney, Chen Zhong,  Awny Alnusair </td>
            </tr>
            <tr>
                <td>11:58 – 12:15</td>
                <td>Paper #146 Robust Weighted Regression for Ultrasound Image Super-Resolution</td>
                <td>Walid Sharabati, Bowei Xi</td>
            </tr>
            <tr>
                <td>12:16 – 12:33</td>
                <td>Paper #150 A Two Layer Pairwise Framework to Approximate Super pixel-based Higher order Conditional Random filed for Semantic Segmentation</td>
                <td>Li Sulimowicz, Ishfaq Ahmad, Alexander Aved </td>
            </tr>
        </table>  
  
 
<br> 
  
  <h3 class="STYLE6"><a name="1" id="1"></a>Motivation </h3>
  <p class="STYLE6"> This workshop focuses on the emerging field of multimedia creation using machine learning (ML) and artificial intelligence (AI) approaches. It aims to bring together researchers from ML and AI and practitioners from multimedia industry to foster multimedia creation. Multimedia creation, including style transfer and image synthesis, have been a major focus of machine learning and AI societies, owing to the recent technological breakthroughs such as generative adversarial networks (GANs). This workshop seeks to reinforce the implications to multimedia creation. It publishes papers on all emerging areas of content understanding and multimedia creation, all traditional areas of computer vision and data mining, and selected areas of artificial intelligence, with a particular emphasis on machine learning for pattern recognition. The applied fields such as art content creation, medical image and signal analysis, massive video/image sequence analysis, facial emotion analysis, control system for automation, content-based retrieval of video and image, and object recognition are also covered. The workshop is expected to provide an interactive platform to researchers, scientists, professors, and students to exchange their innovative ideas and experiences in the areas of Multimedia, and to specialize in the field of multimedia from underlying cutting-edge technologies to applications.</p>
  
<br>
  
  <p class="STYLE6"> We intend to have a half-day workshop with four to five regular talks.</p>
  <p class="STYLE6">&nbsp;</p>
  
  
  <h3 class="STYLE6"><strong><a name="2" id="2"></a>Topics</strong></h3>
  <p class="STYLE6">Potential topics of interest include ML and AI on Multimedia in areas of but not limited to:   </p>
  
  <ul>
  <li class="STYLE6">Generative models for multimedia creation</li>
  <li class="STYLE6">AI for multimedia creation</li>
  <li class="STYLE6">Data mining techniques for multimedia creation</li>
  <li class="STYLE6">Synthesis and prediction of multimedia</li>
  <li class="STYLE6">Deep learning application in video and image analysis</li>
  <li class="STYLE6">Multi-modal data analysis</li>
  <li class="STYLE6">Medical image and signal analysis </li>
  <li class="STYLE6">Content of video and image extraction, analysis and application</li>
  <li class="STYLE6">Online and distributed computing for multimedia creation</li>
  <li class="STYLE6">Wireless technology and demonstrations for multimedia creation</li>
  <li class="STYLE6">Security, privacy and policy regulation for multimedia creation </li>
  <li class="STYLE6">Machine learning on social, emotional and affective multimedia</li>
  <li class="STYLE6">Augmented reality</li>
  <li class="STYLE6">Multimedia applied on control system for automation</li>
  <li class="STYLE6">Human-computer interaction</li>
  <li class="STYLE6">Signal processing including audio, video, image processing, and coding</li>
  <li class="STYLE6">Smart multimedia surveillance</li>
  
  </ul>
  <p class="STYLE6">&nbsp;</p>
  <h3 class="STYLE6"><a name="3" id="3"></a><strong>Paper submission instructions</strong></h3>
  <ul>
  
  <!---<p class="STYLE6"> Please submit a full-length paper (up to 6 pages IEEE 2-column format) through the online submission system  <a href="https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2FICME2018" target="_blank"> here</a>. --->
  
  <li class="STYLE6"> Papers must be no longer than 6 pages, including all text, figures and references. </li>
  <li class="STYLE6"> Only electronic submissions will be accepted through <a href="https://cmt3.research.microsoft.com/ICME2018W" target="_blank">CMT</a> online system. </li>
  
  <li class="STYLE6"> The templates for Microsoft Word and LaTeX submissions are available as below. </li>
  <ul>
  <li class="STYLE6"> 8.5" x 11" Word template downloadable from <a href="http://www.icme2018.org/assets/icme2018template-a950264319d84bd5675833d2b564d29c1eb2e05738f8ee56e58b705467ce60c2.docx" target="_blank"> here</a>. </li>
  <li class="STYLE6"> LaTeX formatting macros downloadable from <a href="http://www.icme2018.org/assets/icme2018template-607e2f9a81f353139e5c3a7a7c7b24d685b12df1d71d81b26bb06a894cfd8340.zip" target="_blank">here</a>. </li>
  </ul>
  
  
  <li class="STYLE6"> For more details, please refer to the submission instruction of ICME 2018 Workshop <a href="http://www.icme2018.org/author_info" target="_blank">here</a>. </li>
  
  </ul>
  
  
  
  <p>&nbsp;</p>
  
  <h3 class="STYLE6"><a name="4" id="4"></a>Important Dates</h3>
  <ul>
  <li class="STYLE6"><b> <del>March 19</del><font color="red"> March 26</font>, 2018:</b> Due date for full workshop papers submission</li>
  <li class="STYLE6"><b> <del>April 23</del><font color="red"> April 27</font>, 2018:</b> Notification of paper acceptance to authors</li>
  <li class="STYLE6"><b> May 11, 2018: </b> Camera-ready of accepted papers</li>
  <li class="STYLE6"><b> July 23-27, 2018:</b> Workshop</li>
  </ul>
  <p></p>
  <p>&nbsp;</p>
  
  <h3 class="STYLE6"><a name="5" id="5"></a>Review procedure</h3>
  <p class="STYLE6"> All submitted paper will be reviewed by 3 program committee members.</p>
  <p>&nbsp;</p>
  
  <h3 class="STYLE6"><a name="6" id="6"></a>Workshop Organizers </h3>
  <p>&nbsp;</p>
  
  <h3 class="STYLE6"><a name="7" id="7"></a>Workshop Chairs</h3>
  <ul><li class="STYLE6">Yanjia Sun, ADP, yanjia.sun@adp.com</li>
  <li class="STYLE6">Sijia Liu, IBM Research AI, sijia.liu@ibm.com</li>
  <li class="STYLE6">Pin-Yu Chen, IBM T. J. Watson Research Center, pin-yu.chen@ibm.com</li>
  </ul>
  <p>&nbsp;</p>
  
  <h3 class="STYLE6"><a name="8" id="8"></a>Technical Program Committee </h3>
  <ul>
  
  <li class="STYLE6">Bhavya Kailkhura, Lawrence Livermore National Laboratory, USA </li>
  <li class="STYLE6">Tianpei Xie, Amazon, USA </li>
  <li class="STYLE6">Yan Yan, University of Michigan, USA </li>
  <li class="STYLE6">Fangrong Peng, Delphi Coorporation, USA</li>
  <li class="STYLE6">Feng Han, Bloomberg, USA</li>
  <li class="STYLE6">Tan Yan, NEC Laboratories America, USA </li>
  <li class="STYLE6">Xin Zhang, Huawei Technologies, USA </li>
  <li class="STYLE6">Renqiang Min, NEC Laboratories America, USA </li>
  <li class="STYLE6">Onur Yilmaz, Nvidia, USA</li>
  <li class="STYLE6">Yanjie Fu, Missouri University of Science and Technology , USA</li>
  <li class="STYLE6">Shuo Chen, The Neat Company, USA </li>
  <li class="STYLE6">Shin-Ming Cheng, National Taiwan University of Science and Technology, Taiwan </li>
  <li class="STYLE6">Baichuan Zhang, Purdue University, USA </li>
  <li class="STYLE6">Xiaoliang Wang, Virginia State University, USA </li>
 
  </ul>    
  
 	
			
  <br>
  
  <h3 class="STYLE6"><a name="9" id="9"></a>Keynote Talk</h3>

            <h3 class="STYLE6" style="color: #0000ff;">A Multi-task Learning framework for Head Pose Estimation and Actor-Action Semantic Video Segmentation</h3>
            <p class="STYLE6"><strong>Prof. Yan Yan, Assistant Professor at Texas State University</strong></p>
            <p class="STYLE6">
                <img src="./image/yanyan.jpg" width="160" style="float: left; margin-left: 0px; margin-right: 8px;margin-top:8px;">
                <strong>Abstract:</strong> Multi-task learning, as one important branch of machine learning, has developed very fast during the past decade. Multi-task learning methods aim to simultaneously learn classification or regression models for a set of related tasks. This typically leads to better models as compared to a learner that does not account for task relationships. In this talk, we will investigate a multi-task learning framework for head pose estimation and actor-action segmentation. (1) Head pose estimation from low-resolution surveillance data has gained in importance. However, monocular and multi-view head pose estimation approaches still work poorly under target motion, as facial appearance distorts owing to camera perspective and scale changes when a person moves around. We propose FEGA-MTL, a novel framework based on multi-task learning for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. Upon partitioning the monitored scene into a dense uniform spatial grid, FEGA-MTL simultaneously clusters grid partitions into regions with similar facial appearance, while learning region-specific head pose classifiers. (2) Fine-grained activity understanding in videos has attracted considerable recent attention with a shift from action classification to detailed actor and action understanding that provides compelling results for perceptual needs of cutting-edge autonomous systems. However, current methods for detailed understanding of actor and action have significant limitations: they require large amounts of finely labeled data, and they fail to capture any internal relationship among actors and actions. To address these issues, we propose a novel, robust multi-task ranking model for weakly-supervised actor-action segmentation where only video-level tags are given for training samples. Our model is able to share useful information among different actors and actions while learning a ranking matrix to select representative supervoxels for actors and actions respectively. 
            </p>
            <p class="STYLE6">	
                <strong>Yan Yan </strong> is currently an Assistant Professor at Texas State University. He was a research fellow at the University of Michigan and at the University of Trento. He received his Ph.D in computer science from the University of Trento Italy, and the M.S. degree from Georgia Institute of Technology. He was a visiting scholar with Carnegie Mellon University in 2013 and a visiting research fellow with the Advanced Digital Sciences Center (ADSC), UIUC, Singapore in 2015. His research interests include computer vision, machine learning, and multimedia. He received the Best Student Paper Award in ICPR 2014 and Best Paper Award in ACM Multimedia 2015. He has published papers in CVPR / ICCV / ECCV / TPAMI / AAAI / IJCAI / ACM Multimedia. He has been PC members for several major conferences and reviewers for referred journals in computer vision and multimedia. He served as a guest editor in TPAMI, CVIU and TOMM. He is a member of the IEEE and the ACM.
            </p>
			
			
			
			
			
 
  <br><br><br><br>
</div>
</body></html>
